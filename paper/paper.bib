@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

@article{lee2018deeptwist,
  title={DeepTwist: Learning Model Compression via Occasional Weight Distortion},
  author={Lee, Dongsoo and Kapoor, Parichay and Kim, Byeongwook},
  journal={arXiv preprint arXiv:1810.12823},
  year={2018}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  booktitle={Advances in neural information processing systems},
  pages={1269--1277},
  year={2014}
}

@inproceedings{bucilua2006model,
  title={Model compression},
  author={Bucilu«é, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006},
  organization={ACM}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amapreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{cheong2014transformer,
  title={transformers.zip: Compressing Transformers with Pruning and Quantization},
  author={Cheong, Robin and Daniel, Robel},
  url={http://web.stanford.edu/class/cs224n/reports/custom/15763707.pdf},
  year={2019}
}

@article{liu2019multi,
  title={Multi-Task Deep Neural Networks for Natural Language Understanding},
  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1901.11504},
  year={2019}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/languageunsupervised/language understanding paper. pdf},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  pages={8},
  year={2019}
}

@article{yang2019fsnet,
  title={FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary},
  author={Yang, Yingzhen and Jojic, Nebojsa and Huan, Jun},
  journal={arXiv preprint arXiv:1902.03264},
  year={2019}
}

@article{mirzadeh2019improved,
  title={Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher},
  author={Mirzadeh, Seyed-Iman and Farajtabar, Mehrdad and Li, Ang and Ghasemzadeh, Hassan},
  journal={arXiv preprint arXiv:1902.03393},
  year={2019}
}
