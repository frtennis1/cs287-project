{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-uncased'\n",
    "num_labels = 2\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "       bert_model, num_labels=num_labels)\n",
    "state_dict_bert = model.state_dict()\n",
    "state_dict_diff_prune = torch.load('output/deeptwist/diff_prune99_2019-04-30_05_42/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_bert = torch.cat([param.flatten() for param in state_dict_bert.values()]).cpu().numpy()\n",
    "flattened_diff_prune = torch.cat([param.flatten() for param in state_dict_diff_prune.values()]).cpu().numpy()\n",
    "diff = abs(flattened_bert - flattened_diff_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010014616046589112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent of parameters that are different\n",
    "np.count_nonzero(diff) / len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = 0\n",
    "rows_list = []\n",
    "for key in state_dict_bert.keys():\n",
    "    shape = state_dict[key].shape\n",
    "    size = state_dict[key].numel()\n",
    "    rows_list.append({'layer': key, 'shape': shape,\n",
    "                     'pct_diff': np.count_nonzero(diff[cursor:cursor + size]) / size})\n",
    "    cursor += size\n",
    "df = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>pct_diff</th>\n",
       "      <th>shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>classifier.weight</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(2, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>classifier.bias</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>(2,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>bert.encoder.layer.7.attention.self.value.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>bert.encoder.layer.6.attention.output.LayerNor...</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>bert.encoder.layer.6.output.dense.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>bert.encoder.layer.6.output.LayerNorm.weight</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>bert.encoder.layer.6.output.LayerNorm.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>bert.encoder.layer.7.attention.self.query.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>bert.encoder.layer.7.attention.self.key.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>bert.encoder.layer.7.attention.output.dense.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>bert.encoder.layer.6.attention.output.dense.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>bert.encoder.layer.7.attention.output.LayerNor...</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>bert.encoder.layer.7.attention.output.LayerNor...</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>bert.encoder.layer.7.output.dense.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>bert.encoder.layer.7.output.LayerNorm.weight</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>bert.encoder.layer.7.output.LayerNorm.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>bert.encoder.layer.8.attention.self.query.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>bert.encoder.layer.6.attention.output.LayerNor...</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>bert.encoder.layer.6.attention.self.value.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>bert.encoder.layer.8.attention.self.value.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>bert.encoder.layer.5.attention.self.key.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>bert.encoder.layer.4.attention.output.LayerNor...</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>bert.encoder.layer.4.attention.output.LayerNor...</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>bert.encoder.layer.4.output.dense.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>bert.encoder.layer.4.output.LayerNorm.weight</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>bert.encoder.layer.4.output.LayerNorm.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>bert.encoder.layer.5.attention.self.query.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>bert.encoder.layer.5.attention.self.value.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>bert.encoder.layer.6.attention.self.key.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>bert.encoder.layer.5.attention.output.dense.bias</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>bert.encoder.layer.2.attention.self.query.weight</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>(768, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>bert.encoder.layer.8.attention.output.dense.we...</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>(768, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>bert.encoder.layer.4.attention.self.key.weight</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>(768, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>bert.encoder.layer.4.attention.output.dense.we...</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>(768, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>bert.encoder.layer.6.attention.output.dense.we...</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>(768, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>bert.encoder.layer.5.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bert.encoder.layer.1.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>bert.encoder.layer.3.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>bert.encoder.layer.11.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>bert.encoder.layer.3.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>bert.encoder.layer.5.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>bert.encoder.layer.4.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>bert.encoder.layer.4.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>bert.encoder.layer.11.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>bert.encoder.layer.10.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bert.encoder.layer.0.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>bert.encoder.layer.10.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bert.encoder.layer.1.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bert.encoder.layer.0.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>bert.encoder.layer.6.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>bert.encoder.layer.6.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>bert.encoder.layer.2.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>bert.encoder.layer.2.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>bert.encoder.layer.7.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>bert.encoder.layer.7.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>bert.encoder.layer.9.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>bert.encoder.layer.9.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>bert.encoder.layer.8.intermediate.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bert.encoder.layer.8.output.dense.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(768, 3072)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert.embeddings.word_embeddings.weight</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>(30522, 768)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 layer  pct_diff         shape\n",
       "199                                  classifier.weight  1.000000      (2, 768)\n",
       "200                                    classifier.bias  0.500000          (2,)\n",
       "122     bert.encoder.layer.7.attention.self.value.bias  0.010417        (768,)\n",
       "110  bert.encoder.layer.6.attention.output.LayerNor...  0.010417        (768,)\n",
       "114             bert.encoder.layer.6.output.dense.bias  0.010417        (768,)\n",
       "115       bert.encoder.layer.6.output.LayerNorm.weight  0.010417        (768,)\n",
       "116         bert.encoder.layer.6.output.LayerNorm.bias  0.010417        (768,)\n",
       "118     bert.encoder.layer.7.attention.self.query.bias  0.010417        (768,)\n",
       "120       bert.encoder.layer.7.attention.self.key.bias  0.010417        (768,)\n",
       "124   bert.encoder.layer.7.attention.output.dense.bias  0.010417        (768,)\n",
       "108   bert.encoder.layer.6.attention.output.dense.bias  0.010417        (768,)\n",
       "125  bert.encoder.layer.7.attention.output.LayerNor...  0.010417        (768,)\n",
       "126  bert.encoder.layer.7.attention.output.LayerNor...  0.010417        (768,)\n",
       "130             bert.encoder.layer.7.output.dense.bias  0.010417        (768,)\n",
       "131       bert.encoder.layer.7.output.LayerNorm.weight  0.010417        (768,)\n",
       "132         bert.encoder.layer.7.output.LayerNorm.bias  0.010417        (768,)\n",
       "134     bert.encoder.layer.8.attention.self.query.bias  0.010417        (768,)\n",
       "109  bert.encoder.layer.6.attention.output.LayerNor...  0.010417        (768,)\n",
       "106     bert.encoder.layer.6.attention.self.value.bias  0.010417        (768,)\n",
       "138     bert.encoder.layer.8.attention.self.value.bias  0.010417        (768,)\n",
       "88        bert.encoder.layer.5.attention.self.key.bias  0.010417        (768,)\n",
       "77   bert.encoder.layer.4.attention.output.LayerNor...  0.010417        (768,)\n",
       "78   bert.encoder.layer.4.attention.output.LayerNor...  0.010417        (768,)\n",
       "82              bert.encoder.layer.4.output.dense.bias  0.010417        (768,)\n",
       "83        bert.encoder.layer.4.output.LayerNorm.weight  0.010417        (768,)\n",
       "84          bert.encoder.layer.4.output.LayerNorm.bias  0.010417        (768,)\n",
       "86      bert.encoder.layer.5.attention.self.query.bias  0.010417        (768,)\n",
       "90      bert.encoder.layer.5.attention.self.value.bias  0.010417        (768,)\n",
       "104       bert.encoder.layer.6.attention.self.key.bias  0.010417        (768,)\n",
       "92    bert.encoder.layer.5.attention.output.dense.bias  0.010417        (768,)\n",
       "..                                                 ...       ...           ...\n",
       "37    bert.encoder.layer.2.attention.self.query.weight  0.010001    (768, 768)\n",
       "139  bert.encoder.layer.8.attention.output.dense.we...  0.010001    (768, 768)\n",
       "71      bert.encoder.layer.4.attention.self.key.weight  0.010001    (768, 768)\n",
       "75   bert.encoder.layer.4.attention.output.dense.we...  0.010001    (768, 768)\n",
       "107  bert.encoder.layer.6.attention.output.dense.we...  0.010001    (768, 768)\n",
       "95      bert.encoder.layer.5.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "31      bert.encoder.layer.1.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "63      bert.encoder.layer.3.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "191    bert.encoder.layer.11.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "65            bert.encoder.layer.3.output.dense.weight  0.010000   (768, 3072)\n",
       "97            bert.encoder.layer.5.output.dense.weight  0.010000   (768, 3072)\n",
       "81            bert.encoder.layer.4.output.dense.weight  0.010000   (768, 3072)\n",
       "79      bert.encoder.layer.4.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "193          bert.encoder.layer.11.output.dense.weight  0.010000   (768, 3072)\n",
       "175    bert.encoder.layer.10.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "15      bert.encoder.layer.0.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "177          bert.encoder.layer.10.output.dense.weight  0.010000   (768, 3072)\n",
       "33            bert.encoder.layer.1.output.dense.weight  0.010000   (768, 3072)\n",
       "17            bert.encoder.layer.0.output.dense.weight  0.010000   (768, 3072)\n",
       "111     bert.encoder.layer.6.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "113           bert.encoder.layer.6.output.dense.weight  0.010000   (768, 3072)\n",
       "49            bert.encoder.layer.2.output.dense.weight  0.010000   (768, 3072)\n",
       "47      bert.encoder.layer.2.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "127     bert.encoder.layer.7.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "129           bert.encoder.layer.7.output.dense.weight  0.010000   (768, 3072)\n",
       "161           bert.encoder.layer.9.output.dense.weight  0.010000   (768, 3072)\n",
       "159     bert.encoder.layer.9.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "143     bert.encoder.layer.8.intermediate.dense.weight  0.010000   (3072, 768)\n",
       "145           bert.encoder.layer.8.output.dense.weight  0.010000   (768, 3072)\n",
       "0               bert.embeddings.word_embeddings.weight  0.010000  (30522, 768)\n",
       "\n",
       "[201 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='pct_diff', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
