%        File: abstract.tex
%     Created: Tue Mar 26 06:00 AM 2019 E
% Last Change: Tue Mar 26 06:00 AM 2019 E
%
\documentclass[11pt]{article}
\usepackage{macros}
\begin{document}

\title{CS287 Final Project Abstract}
\author{Francisco Rivera\thanks{\texttt{frivera@college.harvard.edu}} \and Jiafeng (Kevin) Chen\thanks{\texttt{jiafengchen@college.harvard.edu}} \and Yufeng Ling\thanks{\texttt{yufengling@college.harvard.edu}}}
\date{\today}

\maketitle

\section{Area}

Variational autoencoders (VAE) are a powerful method for learning latent
representations of high-dimensional objects; these representations preserve the
underlying structure of the objects, and have useful features such as
interpolation and natural clustering. In vector-quantized VAE (VQ-VAE), we
discretize the representation space, using the vector quantization algorithm.
Discretizing the representation space effectively compresses inputs, and in some
cases, may lead to interpretable discrete latent codes. We are interested in
using recent advances in vector-quantized variational autoencoding (VQ-VAE) to
improve machine translation, particularly in low-resource settings. 

VQ-VAE has been used \citep{kaiser2018fast} to speed up machine translation.
VQ-VAE achieves this by compressing sentences into shorter discrete
representations, which are then translated using a encoder--decoder framework
(e.g. transformer). The speedup is provided since the variational encoding and
decoding parts are parallelizable. We anticipate that VQ-VAE may also be helpful
in low-resource settings, since variational autoencoding is unsupervised. In
particular, recent advances in VQ-VAE has shown that it can style transfer audio
into being spoken by different speakers, in a fully unsupervised manner
\citep{van2017neural}. Speaker style transfer is a task that bears some
similarities to translation, and we hope to leverage these recent advances for
low-resource machine translation.% TODO: write up area

\section{Papers}
\begin{itemize}
    \item Our main paper is \cite*{van2017neural}, which introduces VQ-VAE.
    VQ-VAE differs from traditional VAEs in two crucial aspects: (1) VQ-VAE is
    discrete, and (2) the prior is learned. 
    \item \cite*{kaiser2018fast}  
    \item \cite{guzman2019two}
\end{itemize}

\section{Time}

We have already signed up for Monday April 22.

\section{Baseline}

We will be using the same Nepali-English and Sinhala-English datasets as in \cite{guzman2019two}. Those languages are only spoken by less than 20 million people in Nepal and Sri Lanka respectively. They are categorized by small amount out-of-domain parallel data and large amount of monolingual data.

With very limited supervised dataset ($\sim$ 500K sentences), the paper achieved BLEU scores of less than 8 for Nepali and Sinhala to English. With a combination of supervised translation data and monolingual data on the target side, BLEU scores increased to 15.1 for both. Finally, unsupervised learning generated BLEU score very close to 0. We intend to use those as baseline and improve upon them.


\bibliographystyle{ecca}
\bibliography{sources}


\end{document}


