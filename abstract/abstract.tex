%        File: abstract.tex
%     Created: Tue Mar 26 06:00 AM 2019 E
% Last Change: Tue Mar 26 06:00 AM 2019 E
%
\documentclass[11pt]{article}
\usepackage{macros}
\begin{document}

\title{CS287 Final Project Abstract}
\author{Francisco Rivera\thanks{\texttt{frivera@college.harvard.edu}} \and Jiafeng (Kevin) Chen\thanks{\texttt{jiafengchen@college.harvard.edu}} \and Yufeng Ling\thanks{\texttt{yufengling@college.harvard.edu}}}
\date{\today}

\maketitle

\section{Area}

Variational autoencoders (VAE) are a powerful method for learning latent
representations of high-dimensional objects; these representations preserve the
underlying structure of the objects, and have useful features such as
interpolation and natural clustering. In vector-quantized VAE (VQ-VAE), we
discretize the representation space, using the vector quantization algorithm.
Discretizing the representation space effectively compresses inputs, and in some
cases, may lead to interpretable discrete latent codes. We are interested in
using recent advances in vector-quantized variational autoencoding (VQ-VAE) to
improve machine translation, particularly in low-resource settings. 

VQ-VAE has
been used \citep{kaiser2018fast} to speed up machine translation. VQ-VAE
achieves this by compressing sentences into shorter discrete representations,
which are then translated using a encoder--decoder framework (e.g. transformer).
The speedup is provided since the variational encoding and decoding parts are
parallelizable. We anticipate that VQ-VAE may also be helpful in low-resource
settings, since variational autoencoding is unsupervised. In particular, recent
advances in VQ-VAE has shown that it can style transfer audio into being spoken
by different
speakers, in a fully unsupervised manner \citep{van2017neural}. Speaker style
transfer is a task that
bears some similarities to translation, and we hope to
leverage these recent advances for low-resource machine translation.
% TODO: write up area

\section{Papers}
\begin{itemize}
    \item Our main paper is \cite*{van2017neural}, which introduces VQ-VAE.
    VQ-VAE differs from traditional VAEs in two crucial aspects: (1) VQ-VAE is
    discrete, and (2) the prior is learned. 
    \item \cite*{kaiser2018fast}  
    \item \cite{guzman2019two}
\end{itemize}



% TODO: bring in the two papers and find a third

\section{Time}

We have already signed up for Monday April 22.

\section{Baseline}

% TODO: this is the hardest thing we need to decide

\bibliographystyle{ecca}
\bibliography{sources}


\end{document}


